[Section]
[Block] (API Documentation: Lexer Module)->(h1) [end]

[Block]
The `Lexer` module is responsible for converting raw input strings into a sequence of meaningful tokens.
[end]

[Block] (Methods)->(h2) [end]

[Block] (Method: lex-source)->(h3) [end]

[Block]
Main entry point for tokenization.
[end]

@_table_@: Parameter, Type, Description;
source, string, The raw SomMark input string;
options, object, Optional configuration for the lexer;
@_end_@

[Block] (Return Value)->(bold)
Returns an array of `Token` objects.
[end]

[Block] (Usage Example)->(h3) [end]

@_code_@: javascript;
import { Lexer } from "sommark";

const tokens = Lexer.lex("[Block] Hello [end]");
console.log(tokens);
/* Output:
  [
    { type: "OPEN_BRACKET", value: "[" },
    { type: "IDENTIFIER", value: "Block" },
    ...
  ]
*/
@_end_@

[Block] (Error Handling)->(h2) [end]

[Block]
The lexer will throw an error if an unescaped special character is found in an invalid context, or if an at-block is not closed properly.
[end]

[Block] (Related Modules)->(h2) [end]
[Block]
(Parser)->(link: #parser, Parser)
(Transpiler)->(link: #transpiler, Transpiler)
[end]

[end]
